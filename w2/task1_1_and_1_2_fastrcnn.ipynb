{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4FGV-HaNYaX",
        "outputId": "5d9b5dc1-92d9-4331-ea4c-73ee19e4b945"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7jbxPjDNeqI",
        "outputId": "3b753321-5b14-4822-c0b8-d8978d576201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "torch:  2.5 ; cuda:  cu124\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "tKKorJJFNrmb",
        "outputId": "0628b061-9253-4281-e4d2-8ed49e0356f0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7bybFWANuWR"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "import torch\n",
        "import shutil\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, DatasetEvaluator\n",
        "from detectron2.data.datasets import register_coco_instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6JFWJomd8H8"
      },
      "source": [
        "### Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FnS9-xnd7W_"
      },
      "outputs": [],
      "source": [
        "def zip_and_download_folder(folder_path, folder_name='output'):\n",
        "  shutil.make_archive(f\"{folder_name}\", \"zip\", folder_path)\n",
        "  files.download(f\"{folder_name}.zip\")\n",
        "  return\n",
        "\n",
        "def unzip_folder(zip_folder_path, unzipped_folder_path):\n",
        "  os.makedirs(unzipped_folder_path, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_folder_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(unzipped_folder_path)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86N1RZecfB7o"
      },
      "outputs": [],
      "source": [
        "# Function to convert YOLO labels to COCO format\n",
        "def yolo_to_coco(yolo_dir, image_dir, output_json):\n",
        "    images = []\n",
        "    annotations = []\n",
        "    annotation_id = 1\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
        "\n",
        "    # Define categories\n",
        "    CATEGORIES = [{\"id\": 2, \"name\": \"car\"}]\n",
        "\n",
        "    for image_id, image_file in enumerate(image_files, start=1):\n",
        "        img_path = os.path.join(image_dir, image_file)\n",
        "        label_path = os.path.join(yolo_dir, os.path.splitext(image_file)[0] + \".txt\")\n",
        "\n",
        "        # Read image dimensions\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        height, width, _ = img.shape\n",
        "\n",
        "        # Add image to COCO structure\n",
        "        images.append({\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": image_file,\n",
        "            \"height\": height,\n",
        "            \"width\": width\n",
        "        })\n",
        "\n",
        "        # Read YOLO annotations\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                category_id = int(parts[0])\n",
        "                x_center, y_center, bbox_width, bbox_height = map(float, parts[1:])\n",
        "\n",
        "                # Convert normalized coords to pixels\n",
        "                x_min = (x_center - bbox_width / 2) * width\n",
        "                y_min = (y_center - bbox_height / 2) * height\n",
        "                bbox_width *= width\n",
        "                bbox_height *= height\n",
        "\n",
        "                # Add annotation to COCO structure\n",
        "                annotations.append({\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"category_id\": category_id,\n",
        "                    \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
        "                    \"area\": bbox_width * bbox_height,\n",
        "                    \"iscrowd\": 0\n",
        "                })\n",
        "                annotation_id += 1\n",
        "\n",
        "    # Create final JSON structure\n",
        "    coco_format = {\n",
        "        \"info\": {\n",
        "            \"description\": \"Dataset converted from YOLO to COCO\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2025,\n",
        "            \"contributor\": \"Conversión Automática\",\n",
        "            \"date_created\": \"2025-03-03\"\n",
        "        },\n",
        "        \"licenses\": [{\"id\": 1, \"name\": \"CC-BY\", \"url\": \"http://creativecommons.org/licenses/by/4.0/\"}],\n",
        "        \"images\": images,\n",
        "        \"annotations\": annotations,\n",
        "        \"categories\": CATEGORIES\n",
        "    }\n",
        "\n",
        "    # Save as JSON\n",
        "    with open(output_json, \"w\") as f:\n",
        "        json.dump(coco_format, f, indent=4)\n",
        "\n",
        "    print(f\"Saved file: {output_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dns2PUICXbfU"
      },
      "source": [
        "# **1.1 Off-the-shelf**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTo3KuDA2I6u"
      },
      "source": [
        "### Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We adapt the dataset from YOLO format. The following steps expect this example structure:\n",
        "\n",
        "```\n",
        "dataset_all\n",
        "└───images\n",
        "|   ├── frame_0001.jpg\n",
        "|   ├── frame_0002.jpg\n",
        "|   ├── ...\n",
        "└───labels\n",
        "    ├── frame_0001.txt\n",
        "    ├── frame_0002.txt\n",
        "    ├── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify this path to your custom directory\n",
        "DATASET_DIR = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGSlqwMTreYb",
        "outputId": "84499e07-acf3-40c8-bf6c-6590cd2956e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved file: /content/drive/MyDrive/annotations.json\n"
          ]
        }
      ],
      "source": [
        "TEST_DIR = \"/content/yolov3/datasets/dataset_all\"\n",
        "IMAGE_DIR_TEST = os.path.join(TEST_DIR, \"images\")\n",
        "LABEL_DIR_TEST = os.path.join(TEST_DIR, \"labels\")\n",
        "\n",
        "unzip_folder(f\"{DATASET_DIR}/dataset_all/labels.zip\", LABEL_DIR_TEST)\n",
        "unzip_folder(f\"{DATASET_DIR}/dataset_all/images.zip\", IMAGE_DIR_TEST)\n",
        "\n",
        "# Convert to COCO\n",
        "yolo_to_coco(LABEL_DIR_TEST, IMAGE_DIR_TEST, f\"{DATASET_DIR}/annotations.json\")\n",
        "dataset_all = 'dataset_all'\n",
        "register_coco_instances(f\"{dataset_all}\", {}, f\"{DATASET_DIR}/annotations.json\", \"/content/yolov3/datasets/dataset_all/images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yovm8kA637ND"
      },
      "source": [
        "### Inference - Model selection: Faster R-CNN\n",
        "\n",
        "Creates output video with predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "TubUAhg43MW5",
        "outputId": "1ff3ddd9-c6bc-41fa-8172-b1762b0ec038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/04 14:01:35 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from model_final.pth ...\n",
            "[03/04 14:01:35 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from model_final.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame width: 1920, height: 1080\n",
            "Output video dimensions: 1920, 1080\n",
            "Inference complete! The output video is saved as: vdo_out_finetune.avi\n",
            "Bounding boxes saved in folder: bbox_output_total\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d4980a32-4ba8-4568-b646-aa1d31c6dc12\", \"bbox_output_video_test.avi_off-the-shelf_confidence0.5.zip\", 969997)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Config model Faster R-CNN\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "checkpointer = DetectionCheckpointer(predictor.model)\n",
        "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "# Set input and output video paths\n",
        "input_video_path = \"vdo.avi\"  # Path to input video\n",
        "output_video_path = \"vdo_out.avi\" # Path to save output video\n",
        "output_txt_folder = \"bbox_output\"  # Path to save bbox txt's\n",
        "os.makedirs(output_txt_folder, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Cannot open input file.\")\n",
        "    exit()\n",
        "\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print(f\"Frame width: {width}, height: {height}\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")  # Codec\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "if not out.isOpened():\n",
        "    print(\"Error: Cannot open output file.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Output video dimensions: {width}, {height}\")\n",
        "\n",
        "frame_number = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    outputs = predictor(frame)\n",
        "\n",
        "    instances = outputs[\"instances\"]\n",
        "    # Filter for the 'car' class (class index 3 in COCO)\n",
        "    car_class_index = 2\n",
        "    car_instances = instances[instances.pred_classes == car_class_index]\n",
        "\n",
        "    bboxes = car_instances.pred_boxes.tensor.cpu().numpy()  # Bounding boxes\n",
        "    scores = car_instances.scores.cpu().numpy()  # Confidence\n",
        "    labels = car_instances.pred_classes.cpu().numpy()  # detected classes\n",
        "\n",
        "    txt_filename = os.path.join(output_txt_folder, f\"frame_{frame_number:06d}.txt\")\n",
        "    with open(txt_filename, \"w\") as f:\n",
        "        for bbox, score, label in zip(bboxes, scores, labels):\n",
        "            x1, y1, x2, y2 = bbox  # Coordenadas del bbox\n",
        "            f.write(f\"{label} {x1} {y1} {x2} {y2} {score}\\n\")\n",
        "\n",
        "    v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0, instance_mode=ColorMode.SEGMENTATION)\n",
        "    v = v.draw_instance_predictions(car_instances.to(\"cpu\"))  # Dibuja clase auto\n",
        "    \n",
        "    processed_frame = v.get_image()[:, :, ::-1]\n",
        "\n",
        "    out.write(processed_frame)\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Inference complete! The output video is saved as:\", output_video_path)\n",
        "print(f\"Bounding boxes saved in folder: {output_txt_folder}\")\n",
        "\n",
        "zip_and_download_folder(f\"{output_txt_folder}\", f\"bbox_output_{input_video_path}_off-the-shelf_confidence{cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NBxJI99rL51"
      },
      "source": [
        "# **1.2 Fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gybzcq99SxYY"
      },
      "source": [
        "## **1.2.1 Prepare the datasets**\n",
        "Convert datasets to COCO JSON format and load the JSON datasets. We adapt the dataset from YOLO format. The following steps expect this example structure:\n",
        "\n",
        "```\n",
        "dataset\n",
        "└───images\n",
        "|   └───train\n",
        "|   |   ├── frame_0001.jpg\n",
        "|   |   ├── frame_0002.jpg\n",
        "|   |   ├── ...\n",
        "|   └───val\n",
        "|       ├── frame_0536.jpg\n",
        "|       ├── frame_0537.jpg\n",
        "|       ├── ...\n",
        "└───labels\n",
        "    └───train\n",
        "    |   ├── frame_0001.txt\n",
        "    |   ├── frame_0002.txt\n",
        "    |   ├── ...\n",
        "    └───val\n",
        "        ├── frame_0536.txt\n",
        "        ├── frame_0537.txt\n",
        "        ├── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3tv94kPc2hd",
        "outputId": "e470ec88-745c-4fb2-aa96-0f3d9e71020a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved file: /content/drive/MyDrive/MCV/datasets/annotations_train.json\n",
            "Saved file: /content/drive/MyDrive/MCV/datasets/annotations_val.json\n",
            "Saved file: /content/drive/MyDrive/MCV/datasets/annotations_test.json\n"
          ]
        }
      ],
      "source": [
        "YOLO_DIR = \"/content/yolov3/datasets\"\n",
        "\n",
        "unzip_folder(\"/content/drive/MyDrive/MCV/datasets/dataset_train.zip\", f\"{YOLO_DIR}/dataset_train\")\n",
        "\n",
        "IMAGE_DIR_TRAIN = os.path.join(YOLO_DIR, \"dataset_train/images/train\")\n",
        "IMAGE_DIR_VAL = os.path.join(YOLO_DIR, \"dataset_train/images/val\")\n",
        "LABEL_DIR_TRAIN = os.path.join(YOLO_DIR, \"dataset_train/labels/train\")\n",
        "LABEL_DIR_VAL = os.path.join(YOLO_DIR, \"dataset_train/labels/val\")\n",
        "\n",
        "IMAGE_DIR_TEST = os.path.join(YOLO_DIR, \"dataset_test/images\")\n",
        "LABEL_DIR_TEST = os.path.join(YOLO_DIR, \"dataset_test/labels\")\n",
        "\n",
        "unzip_folder(\"/content/drive/MyDrive/MCV/datasets/dataset_test/images.zip\", IMAGE_DIR_TEST)\n",
        "unzip_folder(\"/content/drive/MyDrive/MCV/datasets/dataset_test/labels.zip\", LABEL_DIR_TEST)\n",
        "\n",
        "# Ejecutar la conversión para train y val\n",
        "yolo_to_coco(LABEL_DIR_TRAIN, IMAGE_DIR_TRAIN, \"/content/drive/MyDrive/MCV/datasets/annotations_train.json\")\n",
        "yolo_to_coco(LABEL_DIR_VAL, IMAGE_DIR_VAL, \"/content/drive/MyDrive/MCV/datasets/annotations_val.json\")\n",
        "yolo_to_coco(LABEL_DIR_TEST, IMAGE_DIR_TEST, \"/content/drive/MyDrive/MCV/datasets/annotations_test.json\")\n",
        "\n",
        "dataset_train = 'dataset_train'\n",
        "dataset_val = 'dataset_val'\n",
        "dataset_test = 'dataset_test'\n",
        "\n",
        "register_coco_instances(dataset_train, {}, \"/content/drive/MyDrive/MCV/datasets/annotations_train.json\", \"/content/yolov3/datasets/dataset_train/images/train\")\n",
        "register_coco_instances(dataset_val, {}, \"/content/drive/MyDrive/MCV/datasets/annotations_val.json\", \"/content/yolov3/datasets/dataset_train/images/val\")\n",
        "register_coco_instances(dataset_test, {}, \"/content/drive/MyDrive/MCV/datasets/annotations_test.json\", \"/content/yolov3/datasets/dataset_test/images/test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As5cDLDPnCCG"
      },
      "source": [
        "## **1.2.2 Config Fine-Tuning**\n",
        "Adjust the model config to train with our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ZXgiU9pyli8",
        "outputId": "3ceb312d-11e4-4d29-f0b2-7a70ae2bcd65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/03 22:12:32 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [03/03 22:12:32 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[03/03 22:12:32 d2.data.datasets.coco]: Loaded 402 images in COCO format from annotations_train.json\n",
            "[03/03 22:12:32 d2.data.build]: Removed 0 images with no usable annotations. 402 images left.\n",
            "[03/03 22:12:32 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[03/03 22:12:32 d2.data.build]: Using training sampler TrainingSampler\n",
            "[03/03 22:12:32 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[03/03 22:12:32 d2.data.common]: Serializing 402 elements to byte tensors and concatenating them all ...\n",
            "[03/03 22:12:32 d2.data.common]: Serialized dataset takes 0.31 MiB\n",
            "[03/03 22:12:32 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[03/03 22:12:32 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/03 22:12:32 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[03/03 22:12:42 d2.utils.events]:  eta: 0:15:49  iter: 19  total_loss: 1.611  loss_cls: 0.6981  loss_box_reg: 0.8371  loss_rpn_cls: 0.02506  loss_rpn_loc: 0.03936    time: 0.4843  last_time: 0.5628  data_time: 0.0286  last_data_time: 0.0393   lr: 4.9953e-06  max_mem: 2550M\n",
            "[03/03 22:12:52 d2.utils.events]:  eta: 0:15:43  iter: 39  total_loss: 1.62  loss_cls: 0.6682  loss_box_reg: 0.8839  loss_rpn_cls: 0.02943  loss_rpn_loc: 0.04096    time: 0.4797  last_time: 0.4281  data_time: 0.0100  last_data_time: 0.0068   lr: 9.9902e-06  max_mem: 2550M\n",
            "[03/03 22:13:02 d2.utils.events]:  eta: 0:15:49  iter: 59  total_loss: 1.613  loss_cls: 0.6125  loss_box_reg: 0.93  loss_rpn_cls: 0.02827  loss_rpn_loc: 0.03677    time: 0.4894  last_time: 0.5059  data_time: 0.0125  last_data_time: 0.0057   lr: 1.4985e-05  max_mem: 2550M\n",
            "[03/03 22:13:12 d2.utils.events]:  eta: 0:15:40  iter: 79  total_loss: 1.483  loss_cls: 0.5529  loss_box_reg: 0.8516  loss_rpn_cls: 0.02915  loss_rpn_loc: 0.03342    time: 0.4900  last_time: 0.4954  data_time: 0.0110  last_data_time: 0.0112   lr: 1.998e-05  max_mem: 2550M\n",
            "[03/03 22:13:22 d2.utils.events]:  eta: 0:15:30  iter: 99  total_loss: 1.447  loss_cls: 0.4997  loss_box_reg: 0.8663  loss_rpn_cls: 0.02422  loss_rpn_loc: 0.03947    time: 0.4901  last_time: 0.5639  data_time: 0.0143  last_data_time: 0.0059   lr: 2.4975e-05  max_mem: 2551M\n",
            "[03/03 22:13:31 d2.utils.events]:  eta: 0:15:22  iter: 119  total_loss: 1.394  loss_cls: 0.4677  loss_box_reg: 0.8739  loss_rpn_cls: 0.0214  loss_rpn_loc: 0.03552    time: 0.4906  last_time: 0.3849  data_time: 0.0109  last_data_time: 0.0106   lr: 2.997e-05  max_mem: 2551M\n",
            "[03/03 22:13:42 d2.utils.events]:  eta: 0:15:21  iter: 139  total_loss: 1.31  loss_cls: 0.4242  loss_box_reg: 0.8285  loss_rpn_cls: 0.018  loss_rpn_loc: 0.03742    time: 0.4952  last_time: 0.6073  data_time: 0.0121  last_data_time: 0.0058   lr: 3.4965e-05  max_mem: 2552M\n",
            "[03/03 22:13:53 d2.utils.events]:  eta: 0:15:15  iter: 159  total_loss: 1.32  loss_cls: 0.3948  loss_box_reg: 0.8725  loss_rpn_cls: 0.01673  loss_rpn_loc: 0.035    time: 0.5014  last_time: 0.5264  data_time: 0.0150  last_data_time: 0.0054   lr: 3.996e-05  max_mem: 2552M\n",
            "[03/03 22:14:03 d2.utils.events]:  eta: 0:15:13  iter: 179  total_loss: 1.245  loss_cls: 0.3605  loss_box_reg: 0.8398  loss_rpn_cls: 0.01872  loss_rpn_loc: 0.03356    time: 0.5050  last_time: 0.5175  data_time: 0.0149  last_data_time: 0.0241   lr: 4.4955e-05  max_mem: 2552M\n",
            "[03/03 22:14:13 d2.utils.events]:  eta: 0:15:03  iter: 199  total_loss: 1.213  loss_cls: 0.3358  loss_box_reg: 0.7994  loss_rpn_cls: 0.0167  loss_rpn_loc: 0.03616    time: 0.5023  last_time: 0.4502  data_time: 0.0132  last_data_time: 0.0157   lr: 4.995e-05  max_mem: 2552M\n",
            "[03/03 22:14:23 d2.utils.events]:  eta: 0:14:54  iter: 219  total_loss: 1.252  loss_cls: 0.3169  loss_box_reg: 0.8678  loss_rpn_cls: 0.02144  loss_rpn_loc: 0.03952    time: 0.5023  last_time: 0.5111  data_time: 0.0129  last_data_time: 0.0055   lr: 5.4945e-05  max_mem: 2552M\n",
            "[03/03 22:14:34 d2.utils.events]:  eta: 0:14:44  iter: 239  total_loss: 1.148  loss_cls: 0.2792  loss_box_reg: 0.8126  loss_rpn_cls: 0.01708  loss_rpn_loc: 0.03846    time: 0.5041  last_time: 0.4989  data_time: 0.0147  last_data_time: 0.0062   lr: 5.994e-05  max_mem: 2552M\n",
            "[03/03 22:14:44 d2.utils.events]:  eta: 0:14:34  iter: 259  total_loss: 1.156  loss_cls: 0.278  loss_box_reg: 0.8557  loss_rpn_cls: 0.01194  loss_rpn_loc: 0.02926    time: 0.5036  last_time: 0.5359  data_time: 0.0107  last_data_time: 0.0166   lr: 6.4935e-05  max_mem: 2552M\n",
            "[03/03 22:14:53 d2.utils.events]:  eta: 0:14:25  iter: 279  total_loss: 1.054  loss_cls: 0.256  loss_box_reg: 0.7607  loss_rpn_cls: 0.01183  loss_rpn_loc: 0.02881    time: 0.5030  last_time: 0.4530  data_time: 0.0118  last_data_time: 0.0052   lr: 6.993e-05  max_mem: 2552M\n",
            "[03/03 22:15:04 d2.utils.events]:  eta: 0:14:15  iter: 299  total_loss: 1.085  loss_cls: 0.2366  loss_box_reg: 0.7879  loss_rpn_cls: 0.0108  loss_rpn_loc: 0.02889    time: 0.5035  last_time: 0.5139  data_time: 0.0083  last_data_time: 0.0055   lr: 7.4925e-05  max_mem: 2552M\n",
            "[03/03 22:15:14 d2.utils.events]:  eta: 0:14:05  iter: 319  total_loss: 1.02  loss_cls: 0.2207  loss_box_reg: 0.7702  loss_rpn_cls: 0.01182  loss_rpn_loc: 0.02705    time: 0.5037  last_time: 0.4582  data_time: 0.0100  last_data_time: 0.0067   lr: 7.992e-05  max_mem: 2552M\n",
            "[03/03 22:15:24 d2.utils.events]:  eta: 0:13:56  iter: 339  total_loss: 0.958  loss_cls: 0.1998  loss_box_reg: 0.7182  loss_rpn_cls: 0.007877  loss_rpn_loc: 0.0253    time: 0.5036  last_time: 0.5947  data_time: 0.0125  last_data_time: 0.0255   lr: 8.4915e-05  max_mem: 2552M\n",
            "[03/03 22:15:34 d2.utils.events]:  eta: 0:13:48  iter: 359  total_loss: 0.9739  loss_cls: 0.2057  loss_box_reg: 0.72  loss_rpn_cls: 0.01133  loss_rpn_loc: 0.0283    time: 0.5038  last_time: 0.5162  data_time: 0.0120  last_data_time: 0.0056   lr: 8.991e-05  max_mem: 2552M\n",
            "[03/03 22:15:44 d2.utils.events]:  eta: 0:13:39  iter: 379  total_loss: 0.9028  loss_cls: 0.1902  loss_box_reg: 0.6814  loss_rpn_cls: 0.006132  loss_rpn_loc: 0.02623    time: 0.5044  last_time: 0.4280  data_time: 0.0138  last_data_time: 0.0118   lr: 9.4905e-05  max_mem: 2552M\n",
            "[03/03 22:15:55 d2.utils.events]:  eta: 0:13:30  iter: 399  total_loss: 0.8562  loss_cls: 0.1672  loss_box_reg: 0.6309  loss_rpn_cls: 0.006047  loss_rpn_loc: 0.0255    time: 0.5054  last_time: 0.5199  data_time: 0.0111  last_data_time: 0.0059   lr: 9.99e-05  max_mem: 2552M\n",
            "[03/03 22:16:05 d2.utils.events]:  eta: 0:13:21  iter: 419  total_loss: 0.7653  loss_cls: 0.1798  loss_box_reg: 0.5523  loss_rpn_cls: 0.006406  loss_rpn_loc: 0.02846    time: 0.5051  last_time: 0.5860  data_time: 0.0117  last_data_time: 0.0279   lr: 0.0001049  max_mem: 2552M\n",
            "[03/03 22:16:15 d2.utils.events]:  eta: 0:13:12  iter: 439  total_loss: 0.7614  loss_cls: 0.1584  loss_box_reg: 0.5762  loss_rpn_cls: 0.004246  loss_rpn_loc: 0.02511    time: 0.5046  last_time: 0.5101  data_time: 0.0101  last_data_time: 0.0062   lr: 0.00010989  max_mem: 2552M\n",
            "[03/03 22:16:25 d2.utils.events]:  eta: 0:13:02  iter: 459  total_loss: 0.6935  loss_cls: 0.1534  loss_box_reg: 0.4767  loss_rpn_cls: 0.006305  loss_rpn_loc: 0.02864    time: 0.5054  last_time: 0.5079  data_time: 0.0117  last_data_time: 0.0059   lr: 0.00011489  max_mem: 2552M\n",
            "[03/03 22:16:35 d2.utils.events]:  eta: 0:12:53  iter: 479  total_loss: 0.6917  loss_cls: 0.1603  loss_box_reg: 0.4504  loss_rpn_cls: 0.004408  loss_rpn_loc: 0.02581    time: 0.5055  last_time: 0.5118  data_time: 0.0110  last_data_time: 0.0124   lr: 0.00011988  max_mem: 2552M\n",
            "[03/03 22:16:45 d2.utils.events]:  eta: 0:12:42  iter: 499  total_loss: 0.6361  loss_cls: 0.1634  loss_box_reg: 0.4507  loss_rpn_cls: 0.003714  loss_rpn_loc: 0.0256    time: 0.5052  last_time: 0.5806  data_time: 0.0130  last_data_time: 0.0313   lr: 0.00012488  max_mem: 2552M\n",
            "[03/03 22:16:56 d2.utils.events]:  eta: 0:12:33  iter: 519  total_loss: 0.5951  loss_cls: 0.1304  loss_box_reg: 0.419  loss_rpn_cls: 0.003325  loss_rpn_loc: 0.01974    time: 0.5057  last_time: 0.5152  data_time: 0.0111  last_data_time: 0.0056   lr: 0.00012987  max_mem: 2552M\n",
            "[03/03 22:17:06 d2.utils.events]:  eta: 0:12:23  iter: 539  total_loss: 0.6311  loss_cls: 0.1538  loss_box_reg: 0.4323  loss_rpn_cls: 0.004577  loss_rpn_loc: 0.02846    time: 0.5063  last_time: 0.5160  data_time: 0.0137  last_data_time: 0.0065   lr: 0.00013487  max_mem: 2552M\n",
            "[03/03 22:17:16 d2.utils.events]:  eta: 0:12:13  iter: 559  total_loss: 0.5764  loss_cls: 0.139  loss_box_reg: 0.3826  loss_rpn_cls: 0.004167  loss_rpn_loc: 0.02182    time: 0.5066  last_time: 0.4882  data_time: 0.0127  last_data_time: 0.0064   lr: 0.00013986  max_mem: 2552M\n",
            "[03/03 22:17:26 d2.utils.events]:  eta: 0:12:03  iter: 579  total_loss: 0.5338  loss_cls: 0.1267  loss_box_reg: 0.3929  loss_rpn_cls: 0.002827  loss_rpn_loc: 0.02035    time: 0.5062  last_time: 0.4991  data_time: 0.0107  last_data_time: 0.0057   lr: 0.00014486  max_mem: 2552M\n",
            "[03/03 22:17:36 d2.utils.events]:  eta: 0:11:53  iter: 599  total_loss: 0.5184  loss_cls: 0.1338  loss_box_reg: 0.3672  loss_rpn_cls: 0.001714  loss_rpn_loc: 0.0192    time: 0.5060  last_time: 0.4943  data_time: 0.0087  last_data_time: 0.0154   lr: 0.00014985  max_mem: 2552M\n",
            "[03/03 22:17:47 d2.utils.events]:  eta: 0:11:43  iter: 619  total_loss: 0.551  loss_cls: 0.1315  loss_box_reg: 0.3817  loss_rpn_cls: 0.003645  loss_rpn_loc: 0.02147    time: 0.5063  last_time: 0.4887  data_time: 0.0129  last_data_time: 0.0059   lr: 0.00015485  max_mem: 2552M\n",
            "[03/03 22:17:57 d2.utils.events]:  eta: 0:11:33  iter: 639  total_loss: 0.5443  loss_cls: 0.1457  loss_box_reg: 0.3813  loss_rpn_cls: 0.003117  loss_rpn_loc: 0.02281    time: 0.5066  last_time: 0.4954  data_time: 0.0127  last_data_time: 0.0063   lr: 0.00015984  max_mem: 2552M\n",
            "[03/03 22:18:07 d2.utils.events]:  eta: 0:11:23  iter: 659  total_loss: 0.5501  loss_cls: 0.1142  loss_box_reg: 0.3575  loss_rpn_cls: 0.003853  loss_rpn_loc: 0.02085    time: 0.5067  last_time: 0.5866  data_time: 0.0112  last_data_time: 0.0056   lr: 0.00016484  max_mem: 2552M\n",
            "[03/03 22:18:17 d2.utils.events]:  eta: 0:11:13  iter: 679  total_loss: 0.5978  loss_cls: 0.1473  loss_box_reg: 0.4095  loss_rpn_cls: 0.003362  loss_rpn_loc: 0.02061    time: 0.5063  last_time: 0.5159  data_time: 0.0147  last_data_time: 0.0126   lr: 0.00016983  max_mem: 2552M\n",
            "[03/03 22:18:27 d2.utils.events]:  eta: 0:11:03  iter: 699  total_loss: 0.5093  loss_cls: 0.1049  loss_box_reg: 0.3523  loss_rpn_cls: 0.00348  loss_rpn_loc: 0.01792    time: 0.5064  last_time: 0.4552  data_time: 0.0124  last_data_time: 0.0135   lr: 0.00017483  max_mem: 2552M\n",
            "[03/03 22:18:38 d2.utils.events]:  eta: 0:10:53  iter: 719  total_loss: 0.4796  loss_cls: 0.1054  loss_box_reg: 0.319  loss_rpn_cls: 0.002794  loss_rpn_loc: 0.02227    time: 0.5066  last_time: 0.5153  data_time: 0.0112  last_data_time: 0.0055   lr: 0.00017982  max_mem: 2552M\n",
            "[03/03 22:18:47 d2.utils.events]:  eta: 0:10:43  iter: 739  total_loss: 0.4802  loss_cls: 0.1148  loss_box_reg: 0.313  loss_rpn_cls: 0.001873  loss_rpn_loc: 0.02236    time: 0.5059  last_time: 0.5540  data_time: 0.0123  last_data_time: 0.0121   lr: 0.00018482  max_mem: 2552M\n",
            "[03/03 22:18:57 d2.utils.events]:  eta: 0:10:33  iter: 759  total_loss: 0.4653  loss_cls: 0.1163  loss_box_reg: 0.3007  loss_rpn_cls: 0.001462  loss_rpn_loc: 0.01961    time: 0.5059  last_time: 0.3847  data_time: 0.0124  last_data_time: 0.0134   lr: 0.00018981  max_mem: 2552M\n",
            "[03/03 22:19:08 d2.utils.events]:  eta: 0:10:23  iter: 779  total_loss: 0.4998  loss_cls: 0.1214  loss_box_reg: 0.33  loss_rpn_cls: 0.003361  loss_rpn_loc: 0.02282    time: 0.5059  last_time: 0.4543  data_time: 0.0128  last_data_time: 0.0135   lr: 0.00019481  max_mem: 2552M\n",
            "[03/03 22:19:18 d2.utils.events]:  eta: 0:10:13  iter: 799  total_loss: 0.4283  loss_cls: 0.1119  loss_box_reg: 0.2819  loss_rpn_cls: 0.002308  loss_rpn_loc: 0.01825    time: 0.5061  last_time: 0.5172  data_time: 0.0144  last_data_time: 0.0125   lr: 0.0001998  max_mem: 2552M\n",
            "[03/03 22:19:28 d2.utils.events]:  eta: 0:10:03  iter: 819  total_loss: 0.4282  loss_cls: 0.104  loss_box_reg: 0.2809  loss_rpn_cls: 0.001709  loss_rpn_loc: 0.01862    time: 0.5060  last_time: 0.5663  data_time: 0.0082  last_data_time: 0.0136   lr: 0.0002048  max_mem: 2552M\n",
            "[03/03 22:19:38 d2.utils.events]:  eta: 0:09:52  iter: 839  total_loss: 0.4231  loss_cls: 0.109  loss_box_reg: 0.3032  loss_rpn_cls: 0.0007906  loss_rpn_loc: 0.01773    time: 0.5061  last_time: 0.5209  data_time: 0.0136  last_data_time: 0.0150   lr: 0.00020979  max_mem: 2552M\n",
            "[03/03 22:19:49 d2.utils.events]:  eta: 0:09:42  iter: 859  total_loss: 0.4555  loss_cls: 0.1168  loss_box_reg: 0.3281  loss_rpn_cls: 0.001442  loss_rpn_loc: 0.01729    time: 0.5066  last_time: 0.5109  data_time: 0.0125  last_data_time: 0.0054   lr: 0.00021479  max_mem: 2552M\n",
            "[03/03 22:19:59 d2.utils.events]:  eta: 0:09:32  iter: 879  total_loss: 0.4892  loss_cls: 0.1306  loss_box_reg: 0.3427  loss_rpn_cls: 0.002669  loss_rpn_loc: 0.01889    time: 0.5065  last_time: 0.4307  data_time: 0.0159  last_data_time: 0.0065   lr: 0.00021978  max_mem: 2552M\n",
            "[03/03 22:20:09 d2.utils.events]:  eta: 0:09:22  iter: 899  total_loss: 0.4367  loss_cls: 0.09141  loss_box_reg: 0.3291  loss_rpn_cls: 0.001798  loss_rpn_loc: 0.018    time: 0.5067  last_time: 0.5839  data_time: 0.0130  last_data_time: 0.0265   lr: 0.00022478  max_mem: 2552M\n",
            "[03/03 22:20:19 d2.utils.events]:  eta: 0:09:12  iter: 919  total_loss: 0.4611  loss_cls: 0.118  loss_box_reg: 0.3116  loss_rpn_cls: 0.002096  loss_rpn_loc: 0.02036    time: 0.5066  last_time: 0.5196  data_time: 0.0130  last_data_time: 0.0132   lr: 0.00022977  max_mem: 2552M\n",
            "[03/03 22:20:29 d2.utils.events]:  eta: 0:09:02  iter: 939  total_loss: 0.4161  loss_cls: 0.08964  loss_box_reg: 0.2933  loss_rpn_cls: 0.001205  loss_rpn_loc: 0.01577    time: 0.5067  last_time: 0.5204  data_time: 0.0120  last_data_time: 0.0063   lr: 0.00023477  max_mem: 2552M\n",
            "[03/03 22:20:40 d2.utils.events]:  eta: 0:08:52  iter: 959  total_loss: 0.4172  loss_cls: 0.1192  loss_box_reg: 0.284  loss_rpn_cls: 0.000674  loss_rpn_loc: 0.01699    time: 0.5069  last_time: 0.4871  data_time: 0.0120  last_data_time: 0.0058   lr: 0.00023976  max_mem: 2552M\n",
            "[03/03 22:20:50 d2.utils.events]:  eta: 0:08:41  iter: 979  total_loss: 0.426  loss_cls: 0.1058  loss_box_reg: 0.2925  loss_rpn_cls: 0.001383  loss_rpn_loc: 0.01769    time: 0.5069  last_time: 0.5442  data_time: 0.0137  last_data_time: 0.0299   lr: 0.00024476  max_mem: 2552M\n",
            "[03/03 22:21:00 d2.utils.events]:  eta: 0:08:31  iter: 999  total_loss: 0.374  loss_cls: 0.09328  loss_box_reg: 0.2845  loss_rpn_cls: 0.000913  loss_rpn_loc: 0.01521    time: 0.5067  last_time: 0.5144  data_time: 0.0114  last_data_time: 0.0129   lr: 0.00024975  max_mem: 2552M\n",
            "[03/03 22:21:10 d2.utils.events]:  eta: 0:08:21  iter: 1019  total_loss: 0.4103  loss_cls: 0.09639  loss_box_reg: 0.3083  loss_rpn_cls: 0.001342  loss_rpn_loc: 0.01648    time: 0.5066  last_time: 0.5163  data_time: 0.0133  last_data_time: 0.0134   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:21:20 d2.utils.events]:  eta: 0:08:11  iter: 1039  total_loss: 0.3647  loss_cls: 0.08618  loss_box_reg: 0.2628  loss_rpn_cls: 0.0008268  loss_rpn_loc: 0.01538    time: 0.5064  last_time: 0.4554  data_time: 0.0111  last_data_time: 0.0062   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:21:30 d2.utils.events]:  eta: 0:08:01  iter: 1059  total_loss: 0.3625  loss_cls: 0.09715  loss_box_reg: 0.241  loss_rpn_cls: 0.001177  loss_rpn_loc: 0.01597    time: 0.5063  last_time: 0.4682  data_time: 0.0131  last_data_time: 0.0258   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:21:40 d2.utils.events]:  eta: 0:07:51  iter: 1079  total_loss: 0.4116  loss_cls: 0.116  loss_box_reg: 0.2632  loss_rpn_cls: 0.001094  loss_rpn_loc: 0.01749    time: 0.5061  last_time: 0.4486  data_time: 0.0099  last_data_time: 0.0129   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:21:50 d2.utils.events]:  eta: 0:07:41  iter: 1099  total_loss: 0.4036  loss_cls: 0.1103  loss_box_reg: 0.2846  loss_rpn_cls: 0.000989  loss_rpn_loc: 0.01468    time: 0.5062  last_time: 0.4925  data_time: 0.0146  last_data_time: 0.0156   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:00 d2.utils.events]:  eta: 0:07:31  iter: 1119  total_loss: 0.3677  loss_cls: 0.08777  loss_box_reg: 0.2499  loss_rpn_cls: 0.001122  loss_rpn_loc: 0.01445    time: 0.5061  last_time: 0.5049  data_time: 0.0125  last_data_time: 0.0068   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:10 d2.utils.events]:  eta: 0:07:21  iter: 1139  total_loss: 0.3574  loss_cls: 0.09195  loss_box_reg: 0.241  loss_rpn_cls: 0.001465  loss_rpn_loc: 0.01694    time: 0.5058  last_time: 0.5105  data_time: 0.0111  last_data_time: 0.0061   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:20 d2.utils.events]:  eta: 0:07:10  iter: 1159  total_loss: 0.3259  loss_cls: 0.08488  loss_box_reg: 0.2309  loss_rpn_cls: 0.001188  loss_rpn_loc: 0.01489    time: 0.5057  last_time: 0.4522  data_time: 0.0098  last_data_time: 0.0127   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:30 d2.utils.events]:  eta: 0:07:00  iter: 1179  total_loss: 0.3596  loss_cls: 0.08683  loss_box_reg: 0.269  loss_rpn_cls: 0.001273  loss_rpn_loc: 0.01444    time: 0.5058  last_time: 0.4491  data_time: 0.0118  last_data_time: 0.0066   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:40 d2.utils.events]:  eta: 0:06:50  iter: 1199  total_loss: 0.3272  loss_cls: 0.078  loss_box_reg: 0.224  loss_rpn_cls: 0.001141  loss_rpn_loc: 0.01673    time: 0.5061  last_time: 0.5147  data_time: 0.0118  last_data_time: 0.0136   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:22:51 d2.utils.events]:  eta: 0:06:40  iter: 1219  total_loss: 0.3873  loss_cls: 0.09199  loss_box_reg: 0.2405  loss_rpn_cls: 0.001928  loss_rpn_loc: 0.01801    time: 0.5061  last_time: 0.5915  data_time: 0.0123  last_data_time: 0.0188   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:00 d2.utils.events]:  eta: 0:06:29  iter: 1239  total_loss: 0.3226  loss_cls: 0.07211  loss_box_reg: 0.2308  loss_rpn_cls: 0.0007509  loss_rpn_loc: 0.01508    time: 0.5059  last_time: 0.5081  data_time: 0.0128  last_data_time: 0.0058   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:11 d2.utils.events]:  eta: 0:06:19  iter: 1259  total_loss: 0.3564  loss_cls: 0.1085  loss_box_reg: 0.2168  loss_rpn_cls: 0.001101  loss_rpn_loc: 0.01474    time: 0.5061  last_time: 0.4558  data_time: 0.0137  last_data_time: 0.0063   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:21 d2.utils.events]:  eta: 0:06:09  iter: 1279  total_loss: 0.3322  loss_cls: 0.09499  loss_box_reg: 0.2122  loss_rpn_cls: 0.001256  loss_rpn_loc: 0.01546    time: 0.5060  last_time: 0.5168  data_time: 0.0123  last_data_time: 0.0132   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:31 d2.utils.events]:  eta: 0:05:59  iter: 1299  total_loss: 0.3197  loss_cls: 0.06063  loss_box_reg: 0.2353  loss_rpn_cls: 0.0008593  loss_rpn_loc: 0.01234    time: 0.5061  last_time: 0.5749  data_time: 0.0100  last_data_time: 0.0137   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:41 d2.utils.events]:  eta: 0:05:48  iter: 1319  total_loss: 0.2912  loss_cls: 0.06824  loss_box_reg: 0.201  loss_rpn_cls: 0.000783  loss_rpn_loc: 0.01469    time: 0.5062  last_time: 0.5283  data_time: 0.0138  last_data_time: 0.0224   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:23:52 d2.utils.events]:  eta: 0:05:38  iter: 1339  total_loss: 0.3226  loss_cls: 0.07825  loss_box_reg: 0.2271  loss_rpn_cls: 0.0005532  loss_rpn_loc: 0.01374    time: 0.5063  last_time: 0.5162  data_time: 0.0117  last_data_time: 0.0067   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:02 d2.utils.events]:  eta: 0:05:28  iter: 1359  total_loss: 0.3486  loss_cls: 0.09905  loss_box_reg: 0.2402  loss_rpn_cls: 0.0008124  loss_rpn_loc: 0.0173    time: 0.5062  last_time: 0.4994  data_time: 0.0123  last_data_time: 0.0198   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:11 d2.utils.events]:  eta: 0:05:17  iter: 1379  total_loss: 0.3253  loss_cls: 0.07953  loss_box_reg: 0.2327  loss_rpn_cls: 0.001003  loss_rpn_loc: 0.01453    time: 0.5059  last_time: 0.5687  data_time: 0.0121  last_data_time: 0.0207   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:21 d2.utils.events]:  eta: 0:05:07  iter: 1399  total_loss: 0.3402  loss_cls: 0.09279  loss_box_reg: 0.2247  loss_rpn_cls: 0.001398  loss_rpn_loc: 0.01665    time: 0.5056  last_time: 0.5112  data_time: 0.0131  last_data_time: 0.0128   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:31 d2.utils.events]:  eta: 0:04:57  iter: 1419  total_loss: 0.2933  loss_cls: 0.07634  loss_box_reg: 0.2101  loss_rpn_cls: 0.0005506  loss_rpn_loc: 0.01199    time: 0.5056  last_time: 0.5109  data_time: 0.0131  last_data_time: 0.0060   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:42 d2.utils.events]:  eta: 0:04:47  iter: 1439  total_loss: 0.2882  loss_cls: 0.07425  loss_box_reg: 0.1951  loss_rpn_cls: 0.001544  loss_rpn_loc: 0.01313    time: 0.5058  last_time: 0.5162  data_time: 0.0134  last_data_time: 0.0053   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:24:52 d2.utils.events]:  eta: 0:04:36  iter: 1459  total_loss: 0.3177  loss_cls: 0.0851  loss_box_reg: 0.2282  loss_rpn_cls: 0.0007132  loss_rpn_loc: 0.01679    time: 0.5057  last_time: 0.5957  data_time: 0.0130  last_data_time: 0.0273   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:02 d2.utils.events]:  eta: 0:04:26  iter: 1479  total_loss: 0.3162  loss_cls: 0.08899  loss_box_reg: 0.2261  loss_rpn_cls: 0.001351  loss_rpn_loc: 0.01406    time: 0.5058  last_time: 0.5144  data_time: 0.0143  last_data_time: 0.0057   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:12 d2.utils.events]:  eta: 0:04:16  iter: 1499  total_loss: 0.3324  loss_cls: 0.09045  loss_box_reg: 0.2205  loss_rpn_cls: 0.0009279  loss_rpn_loc: 0.01451    time: 0.5059  last_time: 0.5162  data_time: 0.0118  last_data_time: 0.0151   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:22 d2.utils.events]:  eta: 0:04:06  iter: 1519  total_loss: 0.37  loss_cls: 0.1022  loss_box_reg: 0.2432  loss_rpn_cls: 0.001314  loss_rpn_loc: 0.01571    time: 0.5060  last_time: 0.5062  data_time: 0.0128  last_data_time: 0.0149   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:33 d2.utils.events]:  eta: 0:03:55  iter: 1539  total_loss: 0.3307  loss_cls: 0.08174  loss_box_reg: 0.2226  loss_rpn_cls: 0.0007686  loss_rpn_loc: 0.01202    time: 0.5061  last_time: 0.5467  data_time: 0.0118  last_data_time: 0.0060   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:43 d2.utils.events]:  eta: 0:03:45  iter: 1559  total_loss: 0.3082  loss_cls: 0.08707  loss_box_reg: 0.2113  loss_rpn_cls: 0.0009922  loss_rpn_loc: 0.0139    time: 0.5059  last_time: 0.5146  data_time: 0.0138  last_data_time: 0.0064   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:25:53 d2.utils.events]:  eta: 0:03:35  iter: 1579  total_loss: 0.2993  loss_cls: 0.07096  loss_box_reg: 0.2043  loss_rpn_cls: 0.0007266  loss_rpn_loc: 0.01392    time: 0.5059  last_time: 0.4549  data_time: 0.0120  last_data_time: 0.0135   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:03 d2.utils.events]:  eta: 0:03:25  iter: 1599  total_loss: 0.3029  loss_cls: 0.08251  loss_box_reg: 0.1952  loss_rpn_cls: 0.0004859  loss_rpn_loc: 0.01272    time: 0.5059  last_time: 0.4972  data_time: 0.0142  last_data_time: 0.0056   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:13 d2.utils.events]:  eta: 0:03:14  iter: 1619  total_loss: 0.3018  loss_cls: 0.09511  loss_box_reg: 0.1951  loss_rpn_cls: 0.0006232  loss_rpn_loc: 0.01316    time: 0.5057  last_time: 0.4745  data_time: 0.0116  last_data_time: 0.0057   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:23 d2.utils.events]:  eta: 0:03:04  iter: 1639  total_loss: 0.2516  loss_cls: 0.08073  loss_box_reg: 0.1657  loss_rpn_cls: 0.0008932  loss_rpn_loc: 0.01161    time: 0.5058  last_time: 0.5118  data_time: 0.0105  last_data_time: 0.0137   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:33 d2.utils.events]:  eta: 0:02:54  iter: 1659  total_loss: 0.2514  loss_cls: 0.0822  loss_box_reg: 0.1732  loss_rpn_cls: 0.0003545  loss_rpn_loc: 0.01369    time: 0.5058  last_time: 0.5142  data_time: 0.0114  last_data_time: 0.0057   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:43 d2.utils.events]:  eta: 0:02:44  iter: 1679  total_loss: 0.3231  loss_cls: 0.07921  loss_box_reg: 0.2285  loss_rpn_cls: 0.0006048  loss_rpn_loc: 0.01413    time: 0.5059  last_time: 0.5237  data_time: 0.0135  last_data_time: 0.0181   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:26:53 d2.utils.events]:  eta: 0:02:33  iter: 1699  total_loss: 0.259  loss_cls: 0.06827  loss_box_reg: 0.1847  loss_rpn_cls: 0.0006873  loss_rpn_loc: 0.01408    time: 0.5057  last_time: 0.5505  data_time: 0.0106  last_data_time: 0.0132   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:03 d2.utils.events]:  eta: 0:02:23  iter: 1719  total_loss: 0.2894  loss_cls: 0.08452  loss_box_reg: 0.1919  loss_rpn_cls: 0.0004457  loss_rpn_loc: 0.01063    time: 0.5059  last_time: 0.5203  data_time: 0.0149  last_data_time: 0.0057   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:14 d2.utils.events]:  eta: 0:02:13  iter: 1739  total_loss: 0.3319  loss_cls: 0.07521  loss_box_reg: 0.2397  loss_rpn_cls: 0.0006214  loss_rpn_loc: 0.01227    time: 0.5058  last_time: 0.5141  data_time: 0.0134  last_data_time: 0.0130   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:24 d2.utils.events]:  eta: 0:02:03  iter: 1759  total_loss: 0.3419  loss_cls: 0.1052  loss_box_reg: 0.2175  loss_rpn_cls: 0.001203  loss_rpn_loc: 0.01454    time: 0.5058  last_time: 0.5171  data_time: 0.0116  last_data_time: 0.0068   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:33 d2.utils.events]:  eta: 0:01:52  iter: 1779  total_loss: 0.3138  loss_cls: 0.08256  loss_box_reg: 0.2036  loss_rpn_cls: 0.0008189  loss_rpn_loc: 0.01219    time: 0.5055  last_time: 0.5011  data_time: 0.0103  last_data_time: 0.0061   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:44 d2.utils.events]:  eta: 0:01:42  iter: 1799  total_loss: 0.2929  loss_cls: 0.09401  loss_box_reg: 0.1907  loss_rpn_cls: 0.00124  loss_rpn_loc: 0.01377    time: 0.5057  last_time: 0.5212  data_time: 0.0136  last_data_time: 0.0303   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:27:54 d2.utils.events]:  eta: 0:01:32  iter: 1819  total_loss: 0.2916  loss_cls: 0.07606  loss_box_reg: 0.208  loss_rpn_cls: 0.0006707  loss_rpn_loc: 0.01672    time: 0.5059  last_time: 0.5123  data_time: 0.0116  last_data_time: 0.0134   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:04 d2.utils.events]:  eta: 0:01:22  iter: 1839  total_loss: 0.303  loss_cls: 0.08506  loss_box_reg: 0.2011  loss_rpn_cls: 0.0005685  loss_rpn_loc: 0.01277    time: 0.5058  last_time: 0.4544  data_time: 0.0131  last_data_time: 0.0124   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:14 d2.utils.events]:  eta: 0:01:11  iter: 1859  total_loss: 0.278  loss_cls: 0.09143  loss_box_reg: 0.1955  loss_rpn_cls: 0.0003882  loss_rpn_loc: 0.01385    time: 0.5057  last_time: 0.5744  data_time: 0.0130  last_data_time: 0.0272   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:24 d2.utils.events]:  eta: 0:01:01  iter: 1879  total_loss: 0.2897  loss_cls: 0.06881  loss_box_reg: 0.1862  loss_rpn_cls: 0.0009106  loss_rpn_loc: 0.01097    time: 0.5058  last_time: 0.5153  data_time: 0.0131  last_data_time: 0.0126   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:35 d2.utils.events]:  eta: 0:00:51  iter: 1899  total_loss: 0.2877  loss_cls: 0.07194  loss_box_reg: 0.1939  loss_rpn_cls: 0.0007481  loss_rpn_loc: 0.01342    time: 0.5059  last_time: 0.4908  data_time: 0.0118  last_data_time: 0.0075   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:45 d2.utils.events]:  eta: 0:00:41  iter: 1919  total_loss: 0.2711  loss_cls: 0.07628  loss_box_reg: 0.1828  loss_rpn_cls: 0.0006444  loss_rpn_loc: 0.01655    time: 0.5060  last_time: 0.4456  data_time: 0.0123  last_data_time: 0.0136   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:28:55 d2.utils.events]:  eta: 0:00:30  iter: 1939  total_loss: 0.2568  loss_cls: 0.07513  loss_box_reg: 0.1745  loss_rpn_cls: 0.0004584  loss_rpn_loc: 0.01165    time: 0.5058  last_time: 0.5848  data_time: 0.0105  last_data_time: 0.0176   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:29:05 d2.utils.events]:  eta: 0:00:20  iter: 1959  total_loss: 0.2842  loss_cls: 0.06348  loss_box_reg: 0.1956  loss_rpn_cls: 0.0004662  loss_rpn_loc: 0.01069    time: 0.5059  last_time: 0.5049  data_time: 0.0123  last_data_time: 0.0067   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:29:15 d2.utils.events]:  eta: 0:00:10  iter: 1979  total_loss: 0.2959  loss_cls: 0.07193  loss_box_reg: 0.1907  loss_rpn_cls: 0.0003723  loss_rpn_loc: 0.0155    time: 0.5060  last_time: 0.5210  data_time: 0.0132  last_data_time: 0.0068   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:29:30 d2.utils.events]:  eta: 0:00:00  iter: 1999  total_loss: 0.2844  loss_cls: 0.08137  loss_box_reg: 0.1947  loss_rpn_cls: 0.0009049  loss_rpn_loc: 0.01252    time: 0.5060  last_time: 0.4235  data_time: 0.0104  last_data_time: 0.0072   lr: 0.00025  max_mem: 2552M\n",
            "[03/03 22:29:30 d2.engine.hooks]: Overall training speed: 1998 iterations in 0:16:51 (0.5061 s / it)\n",
            "[03/03 22:29:30 d2.engine.hooks]: Total training time: 0:16:56 (0:00:05 on hooks)\n",
            "WARNING [03/03 22:29:30 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[03/03 22:29:30 d2.data.datasets.coco]: Loaded 133 images in COCO format from annotations_val.json\n",
            "[03/03 22:29:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[03/03 22:29:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[03/03 22:29:30 d2.data.common]: Serializing 133 elements to byte tensors and concatenating them all ...\n",
            "[03/03 22:29:30 d2.data.common]: Serialized dataset takes 0.10 MiB\n",
            "WARNING [03/03 22:29:30 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "[03/03 22:29:30 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "WARNING [03/03 22:29:30 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "WARNING [03/03 22:29:30 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[03/03 22:29:30 d2.data.datasets.coco]: Loaded 133 images in COCO format from annotations_val.json\n",
            "[03/03 22:29:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[03/03 22:29:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[03/03 22:29:30 d2.data.common]: Serializing 133 elements to byte tensors and concatenating them all ...\n",
            "[03/03 22:29:30 d2.data.common]: Serialized dataset takes 0.10 MiB\n",
            "[03/03 22:29:30 d2.evaluation.evaluator]: Start inference on 133 batches\n",
            "[03/03 22:29:32 d2.evaluation.evaluator]: Inference done 11/133. Dataloading: 0.0034 s/iter. Inference: 0.1175 s/iter. Eval: 0.0003 s/iter. Total: 0.1212 s/iter. ETA=0:00:14\n",
            "[03/03 22:29:37 d2.evaluation.evaluator]: Inference done 48/133. Dataloading: 0.0082 s/iter. Inference: 0.1277 s/iter. Eval: 0.0005 s/iter. Total: 0.1366 s/iter. ETA=0:00:11\n",
            "[03/03 22:29:42 d2.evaluation.evaluator]: Inference done 84/133. Dataloading: 0.0084 s/iter. Inference: 0.1296 s/iter. Eval: 0.0004 s/iter. Total: 0.1386 s/iter. ETA=0:00:06\n",
            "[03/03 22:29:47 d2.evaluation.evaluator]: Inference done 125/133. Dataloading: 0.0063 s/iter. Inference: 0.1261 s/iter. Eval: 0.0004 s/iter. Total: 0.1329 s/iter. ETA=0:00:01\n",
            "[03/03 22:29:48 d2.evaluation.evaluator]: Total inference time: 0:00:16.994270 (0.132768 s / iter per device, on 1 devices)\n",
            "[03/03 22:29:48 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:16 (0.125717 s / iter per device, on 1 devices)\n",
            "[03/03 22:29:48 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[03/03 22:29:48 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[03/03 22:29:48 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.61s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.877\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.949\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.856\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.974\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.096\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.838\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.895\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.878\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.980\n",
            "[03/03 22:29:49 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 87.668 | 97.948 | 94.885 |  nan  | 85.558 | 97.415 |\n",
            "[03/03 22:29:49 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[03/03 22:29:49 d2.engine.defaults]: Evaluation results for dataset_val in csv format:\n",
            "[03/03 22:29:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[03/03 22:29:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[03/03 22:29:49 d2.evaluation.testing]: copypaste: 87.6683,97.9482,94.8854,nan,85.5582,97.4148\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b602178e-3b65-47fc-8acf-8c5daf111347\", \"output.zip\", 308145616)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Config model and training\n",
        "cfg = get_cfg()\n",
        "model_name = \"faster_rcnn_R_50_FPN_3x\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (f\"{dataset_train}\",)\n",
        "cfg.DATASETS.TEST = (f\"{dataset_val}\",)\n",
        "cfg.MODEL.DEVICE = \"cuda\"\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Num classes of our dataset\n",
        "cfg.MODEL.BACKBONE.FREEZE_AT = 2  # Freeze initial layers\n",
        "# cfg.TEST.EVAL_PERIOD = 50  # Eval every 50 iters\n",
        "\n",
        "cfg.OUTPUT_DIR = f\"./output/fine-tuning/{model_name}\"\n",
        "\n",
        "# Train\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate\n",
        "trainer.test(cfg, trainer.model, evaluators=[COCOEvaluator(f\"{dataset_val}\", cfg, True, cfg.OUTPUT_DIR)])  # Evaluamos al final\n",
        "\n",
        "zip_and_download_folder(cfg.OUTPUT_DIR, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smMoyW7WF6Qt"
      },
      "source": [
        "## **1.2.3 Evaluate the model**\n",
        "After training, evaluate over the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2t50xIYGFbg",
        "outputId": "66ec0daf-4642-47a1-cb71-0c75432f02e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/03 22:32:03 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "WARNING [03/03 22:32:03 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "WARNING [03/03 22:32:03 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[03/03 22:32:03 d2.data.datasets.coco]: Loaded 133 images in COCO format from annotations_val.json\n",
            "[03/03 22:32:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[03/03 22:32:03 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[03/03 22:32:03 d2.data.common]: Serializing 133 elements to byte tensors and concatenating them all ...\n",
            "[03/03 22:32:03 d2.data.common]: Serialized dataset takes 0.10 MiB\n",
            "[03/03 22:32:03 d2.evaluation.evaluator]: Start inference on 133 batches\n",
            "[03/03 22:32:05 d2.evaluation.evaluator]: Inference done 11/133. Dataloading: 0.0211 s/iter. Inference: 0.1389 s/iter. Eval: 0.0013 s/iter. Total: 0.1613 s/iter. ETA=0:00:19\n",
            "[03/03 22:32:10 d2.evaluation.evaluator]: Inference done 48/133. Dataloading: 0.0107 s/iter. Inference: 0.1296 s/iter. Eval: 0.0005 s/iter. Total: 0.1410 s/iter. ETA=0:00:11\n",
            "[03/03 22:32:15 d2.evaluation.evaluator]: Inference done 89/133. Dataloading: 0.0067 s/iter. Inference: 0.1245 s/iter. Eval: 0.0004 s/iter. Total: 0.1318 s/iter. ETA=0:00:05\n",
            "[03/03 22:32:20 d2.evaluation.evaluator]: Inference done 126/133. Dataloading: 0.0068 s/iter. Inference: 0.1259 s/iter. Eval: 0.0004 s/iter. Total: 0.1332 s/iter. ETA=0:00:00\n",
            "[03/03 22:32:21 d2.evaluation.evaluator]: Total inference time: 0:00:17.179260 (0.134213 s / iter per device, on 1 devices)\n",
            "[03/03 22:32:21 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:16 (0.126119 s / iter per device, on 1 devices)\n",
            "[03/03 22:32:21 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[03/03 22:32:21 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[03/03 22:32:22 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.877\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.949\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.856\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.974\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.096\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.838\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.895\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.878\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.980\n",
            "[03/03 22:32:22 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 87.668 | 97.948 | 94.885 |  nan  | 85.558 | 97.415 |\n",
            "[03/03 22:32:22 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "OrderedDict([('bbox', {'AP': 87.66831977619974, 'AP50': 97.94817449339192, 'AP75': 94.88538952992447, 'APs': nan, 'APm': 85.55822602410844, 'APl': 97.41482880004551})])\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "evaluator = COCOEvaluator(f\"{dataset_val}\", cfg, False, output_dir=f\"./output/fine-tuning/eval/{model_name}\")\n",
        "val_loader = build_detection_test_loader(cfg, f\"{dataset_val}\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdwWeidRKmxJ"
      },
      "source": [
        "## **1.2.3 Use the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJE9026zKqNc",
        "outputId": "7b6dbb6e-96f5-47c8-9af8-970f55d4ce78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/03 22:34:57 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        }
      ],
      "source": [
        "cfg.MODEL.WEIGHTS = \"./output/model_final.pth\"  # Load trained weights\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0SFUjY4LBMd"
      },
      "outputs": [],
      "source": [
        "# Path to input video and output files\n",
        "input_video_path = \"vdo.avi\"\n",
        "output_video_path = \"vdo_out.avi\"\n",
        "output_txt_folder = \"bbox_output\"\n",
        "os.makedirs(output_txt_folder, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Cannot open input file.\")\n",
        "    exit()\n",
        "\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print(f\"Frame width: {width}, height: {height}\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")  # Codec\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "if not out.isOpened():\n",
        "    print(\"Error: Cannot open output file.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Output video dimensions: {width}, {height}\")\n",
        "\n",
        "frame_number = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    outputs = predictor(frame)\n",
        "\n",
        "    instances = outputs[\"instances\"]\n",
        "    bboxes = instances.pred_boxes.tensor.cpu().numpy()  # Bounding boxes\n",
        "    scores = instances.scores.cpu().numpy()  # Confdence\n",
        "    labels = instances.pred_classes.cpu().numpy()  # Detected classes\n",
        "\n",
        "    txt_filename = os.path.join(output_txt_folder, f\"frame_{frame_number:06d}.txt\")\n",
        "    with open(txt_filename, \"w\") as f:\n",
        "        for bbox, score, label in zip(bboxes, scores, labels):\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            f.write(f\"{label} {x1} {y1} {x2} {y2} {score}\\n\")\n",
        "\n",
        "    v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)\n",
        "    v = v.draw_instance_predictions(car_instances.to(\"cpu\"))  # Dibuja clase auto\n",
        "\n",
        "    processed_frame = v.get_image()[:, :, ::-1]\n",
        "\n",
        "    out.write(processed_frame)\n",
        "\n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Inference complete! The output video is saved as:\", output_video_path)\n",
        "print(f\"Bounding boxes saved in folder: {output_txt_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRAfyt_TTtj_"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model over the test set\n",
        "evaluator = COCOEvaluator(\"dataset_test\", cfg, False, output_dir=\"./output/\")\n",
        "test_loader = build_detection_test_loader(cfg, \"dataset_test\")\n",
        "print(inference_on_dataset(trainer.model, test_loader, evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXHnLqPtfhL3",
        "outputId": "b72d874e-d8f3-469b-8345-348592675e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/04 00:13:42 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/drive/MyDrive/MCV/model_final.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03/04 00:13:42 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "WARNING [03/04 00:13:42 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "WARNING [03/04 00:13:43 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[03/04 00:13:43 d2.data.datasets.coco]: Loaded 1606 images in COCO format from /content/drive/MyDrive/MCV/annotations_test.json\n",
            "[03/04 00:13:43 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[03/04 00:13:43 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[03/04 00:13:43 d2.data.common]: Serializing 1606 elements to byte tensors and concatenating them all ...\n",
            "[03/04 00:13:43 d2.data.common]: Serialized dataset takes 1.24 MiB\n",
            "[03/04 00:13:43 d2.evaluation.evaluator]: Start inference on 1606 batches\n",
            "[03/04 00:13:45 d2.evaluation.evaluator]: Inference done 11/1606. Dataloading: 0.0175 s/iter. Inference: 0.1360 s/iter. Eval: 0.0003 s/iter. Total: 0.1539 s/iter. ETA=0:04:05\n",
            "[03/04 00:13:50 d2.evaluation.evaluator]: Inference done 43/1606. Dataloading: 0.0233 s/iter. Inference: 0.1334 s/iter. Eval: 0.0005 s/iter. Total: 0.1578 s/iter. ETA=0:04:06\n",
            "[03/04 00:13:55 d2.evaluation.evaluator]: Inference done 82/1606. Dataloading: 0.0164 s/iter. Inference: 0.1259 s/iter. Eval: 0.0006 s/iter. Total: 0.1433 s/iter. ETA=0:03:38\n",
            "[03/04 00:14:00 d2.evaluation.evaluator]: Inference done 119/1606. Dataloading: 0.0156 s/iter. Inference: 0.1249 s/iter. Eval: 0.0006 s/iter. Total: 0.1413 s/iter. ETA=0:03:30\n",
            "[03/04 00:14:05 d2.evaluation.evaluator]: Inference done 156/1606. Dataloading: 0.0156 s/iter. Inference: 0.1241 s/iter. Eval: 0.0006 s/iter. Total: 0.1405 s/iter. ETA=0:03:23\n",
            "[03/04 00:14:10 d2.evaluation.evaluator]: Inference done 197/1606. Dataloading: 0.0138 s/iter. Inference: 0.1222 s/iter. Eval: 0.0005 s/iter. Total: 0.1367 s/iter. ETA=0:03:12\n",
            "[03/04 00:14:15 d2.evaluation.evaluator]: Inference done 231/1606. Dataloading: 0.0150 s/iter. Inference: 0.1227 s/iter. Eval: 0.0005 s/iter. Total: 0.1384 s/iter. ETA=0:03:10\n",
            "[03/04 00:14:20 d2.evaluation.evaluator]: Inference done 270/1606. Dataloading: 0.0138 s/iter. Inference: 0.1224 s/iter. Eval: 0.0005 s/iter. Total: 0.1369 s/iter. ETA=0:03:02\n",
            "[03/04 00:14:25 d2.evaluation.evaluator]: Inference done 311/1606. Dataloading: 0.0125 s/iter. Inference: 0.1218 s/iter. Eval: 0.0005 s/iter. Total: 0.1350 s/iter. ETA=0:02:54\n",
            "[03/04 00:14:30 d2.evaluation.evaluator]: Inference done 343/1606. Dataloading: 0.0138 s/iter. Inference: 0.1225 s/iter. Eval: 0.0005 s/iter. Total: 0.1370 s/iter. ETA=0:02:53\n",
            "[03/04 00:14:35 d2.evaluation.evaluator]: Inference done 384/1606. Dataloading: 0.0129 s/iter. Inference: 0.1219 s/iter. Eval: 0.0005 s/iter. Total: 0.1355 s/iter. ETA=0:02:45\n",
            "[03/04 00:14:40 d2.evaluation.evaluator]: Inference done 423/1606. Dataloading: 0.0125 s/iter. Inference: 0.1219 s/iter. Eval: 0.0005 s/iter. Total: 0.1351 s/iter. ETA=0:02:39\n",
            "[03/04 00:14:45 d2.evaluation.evaluator]: Inference done 453/1606. Dataloading: 0.0139 s/iter. Inference: 0.1227 s/iter. Eval: 0.0005 s/iter. Total: 0.1373 s/iter. ETA=0:02:38\n",
            "[03/04 00:14:50 d2.evaluation.evaluator]: Inference done 485/1606. Dataloading: 0.0145 s/iter. Inference: 0.1235 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:02:35\n",
            "[03/04 00:14:55 d2.evaluation.evaluator]: Inference done 524/1606. Dataloading: 0.0141 s/iter. Inference: 0.1235 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:02:29\n",
            "[03/04 00:15:00 d2.evaluation.evaluator]: Inference done 559/1606. Dataloading: 0.0144 s/iter. Inference: 0.1236 s/iter. Eval: 0.0005 s/iter. Total: 0.1387 s/iter. ETA=0:02:25\n",
            "[03/04 00:15:06 d2.evaluation.evaluator]: Inference done 599/1606. Dataloading: 0.0139 s/iter. Inference: 0.1234 s/iter. Eval: 0.0005 s/iter. Total: 0.1379 s/iter. ETA=0:02:18\n",
            "[03/04 00:15:11 d2.evaluation.evaluator]: Inference done 634/1606. Dataloading: 0.0139 s/iter. Inference: 0.1236 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:02:14\n",
            "[03/04 00:15:16 d2.evaluation.evaluator]: Inference done 672/1606. Dataloading: 0.0137 s/iter. Inference: 0.1236 s/iter. Eval: 0.0005 s/iter. Total: 0.1380 s/iter. ETA=0:02:08\n",
            "[03/04 00:15:21 d2.evaluation.evaluator]: Inference done 712/1606. Dataloading: 0.0132 s/iter. Inference: 0.1235 s/iter. Eval: 0.0005 s/iter. Total: 0.1373 s/iter. ETA=0:02:02\n",
            "[03/04 00:15:26 d2.evaluation.evaluator]: Inference done 744/1606. Dataloading: 0.0136 s/iter. Inference: 0.1239 s/iter. Eval: 0.0005 s/iter. Total: 0.1382 s/iter. ETA=0:01:59\n",
            "[03/04 00:15:31 d2.evaluation.evaluator]: Inference done 783/1606. Dataloading: 0.0132 s/iter. Inference: 0.1238 s/iter. Eval: 0.0005 s/iter. Total: 0.1377 s/iter. ETA=0:01:53\n",
            "[03/04 00:15:36 d2.evaluation.evaluator]: Inference done 823/1606. Dataloading: 0.0128 s/iter. Inference: 0.1242 s/iter. Eval: 0.0005 s/iter. Total: 0.1377 s/iter. ETA=0:01:47\n",
            "[03/04 00:15:41 d2.evaluation.evaluator]: Inference done 857/1606. Dataloading: 0.0128 s/iter. Inference: 0.1247 s/iter. Eval: 0.0005 s/iter. Total: 0.1381 s/iter. ETA=0:01:43\n",
            "[03/04 00:15:46 d2.evaluation.evaluator]: Inference done 897/1606. Dataloading: 0.0124 s/iter. Inference: 0.1245 s/iter. Eval: 0.0005 s/iter. Total: 0.1376 s/iter. ETA=0:01:37\n",
            "[03/04 00:15:51 d2.evaluation.evaluator]: Inference done 934/1606. Dataloading: 0.0123 s/iter. Inference: 0.1246 s/iter. Eval: 0.0005 s/iter. Total: 0.1375 s/iter. ETA=0:01:32\n",
            "[03/04 00:15:56 d2.evaluation.evaluator]: Inference done 969/1606. Dataloading: 0.0125 s/iter. Inference: 0.1247 s/iter. Eval: 0.0005 s/iter. Total: 0.1378 s/iter. ETA=0:01:27\n",
            "[03/04 00:16:01 d2.evaluation.evaluator]: Inference done 1009/1606. Dataloading: 0.0122 s/iter. Inference: 0.1245 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:01:21\n",
            "[03/04 00:16:07 d2.evaluation.evaluator]: Inference done 1045/1606. Dataloading: 0.0121 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:01:17\n",
            "[03/04 00:16:12 d2.evaluation.evaluator]: Inference done 1082/1606. Dataloading: 0.0121 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:01:12\n",
            "[03/04 00:16:17 d2.evaluation.evaluator]: Inference done 1122/1606. Dataloading: 0.0120 s/iter. Inference: 0.1245 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:01:06\n",
            "[03/04 00:16:22 d2.evaluation.evaluator]: Inference done 1155/1606. Dataloading: 0.0121 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:01:02\n",
            "[03/04 00:16:27 d2.evaluation.evaluator]: Inference done 1193/1606. Dataloading: 0.0121 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1373 s/iter. ETA=0:00:56\n",
            "[03/04 00:16:32 d2.evaluation.evaluator]: Inference done 1233/1606. Dataloading: 0.0119 s/iter. Inference: 0.1245 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:00:51\n",
            "[03/04 00:16:37 d2.evaluation.evaluator]: Inference done 1265/1606. Dataloading: 0.0121 s/iter. Inference: 0.1248 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:00:46\n",
            "[03/04 00:16:42 d2.evaluation.evaluator]: Inference done 1305/1606. Dataloading: 0.0118 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:41\n",
            "[03/04 00:16:47 d2.evaluation.evaluator]: Inference done 1344/1606. Dataloading: 0.0117 s/iter. Inference: 0.1247 s/iter. Eval: 0.0004 s/iter. Total: 0.1370 s/iter. ETA=0:00:35\n",
            "[03/04 00:16:52 d2.evaluation.evaluator]: Inference done 1377/1606. Dataloading: 0.0118 s/iter. Inference: 0.1250 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:31\n",
            "[03/04 00:16:57 d2.evaluation.evaluator]: Inference done 1417/1606. Dataloading: 0.0117 s/iter. Inference: 0.1248 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:25\n",
            "[03/04 00:17:02 d2.evaluation.evaluator]: Inference done 1454/1606. Dataloading: 0.0116 s/iter. Inference: 0.1249 s/iter. Eval: 0.0004 s/iter. Total: 0.1371 s/iter. ETA=0:00:20\n",
            "[03/04 00:17:07 d2.evaluation.evaluator]: Inference done 1487/1606. Dataloading: 0.0118 s/iter. Inference: 0.1250 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:16\n",
            "[03/04 00:17:12 d2.evaluation.evaluator]: Inference done 1527/1606. Dataloading: 0.0117 s/iter. Inference: 0.1248 s/iter. Eval: 0.0004 s/iter. Total: 0.1372 s/iter. ETA=0:00:10\n",
            "[03/04 00:17:18 d2.evaluation.evaluator]: Inference done 1562/1606. Dataloading: 0.0119 s/iter. Inference: 0.1250 s/iter. Eval: 0.0004 s/iter. Total: 0.1375 s/iter. ETA=0:00:06\n",
            "[03/04 00:17:23 d2.evaluation.evaluator]: Inference done 1599/1606. Dataloading: 0.0118 s/iter. Inference: 0.1250 s/iter. Eval: 0.0004 s/iter. Total: 0.1374 s/iter. ETA=0:00:00\n",
            "[03/04 00:17:24 d2.evaluation.evaluator]: Total inference time: 0:03:39.971747 (0.137396 s / iter per device, on 1 devices)\n",
            "[03/04 00:17:24 d2.evaluation.evaluator]: Total inference pure compute time: 0:03:20 (0.124962 s / iter per device, on 1 devices)\n",
            "[03/04 00:17:24 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[03/04 00:17:24 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[03/04 00:17:24 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=3.49s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.22s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.832\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.959\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.916\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.809\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.922\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.098\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.840\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.857\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.839\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.933\n",
            "[03/04 00:17:28 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 83.227 | 95.947 | 91.589 |  nan  | 80.888 | 92.245 |\n",
            "[03/04 00:17:28 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "Predicciones guardadas en: output/predictions.json\n"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cuda\"\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Ajust tu the num of classes of our dataset\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/MCV/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "# Construct the model manually instead of using 'trainer'\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Eval over the test set\n",
        "evaluator = COCOEvaluator(\"dataset_test\", cfg, False, output_dir=\"./output/\")\n",
        "test_loader = build_detection_test_loader(cfg, \"dataset_test\")\n",
        "# print(inference_on_dataset(model, test_loader, evaluator))\n",
        "\n",
        "results = inference_on_dataset(model, test_loader, evaluator)\n",
        "output_path = \"output/results.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"results saved at: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
